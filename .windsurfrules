## Enhanced & Contextualized Strict Rules for AI Assistant (Cursor) Manipulating the Suna Project for Leak.la (comercialtoddy)

**Project:** Suna (kortix-ai/suna), as adapted and utilized by Leak.la.

**Startup Context:** Leak.la (comercialtoddy) is dedicated to leveraging cutting-edge AI solutions like the Suna project to drive innovation, deliver exceptional user experiences, and maintain the highest standards of quality and security. These rules are designed to ensure the AI assistant (Cursor) operates in full alignment with Leak.la's objectives and technical best practices.

**Objective:** To ensure consistent, secure, high-quality, and strategically-aligned modifications, implementations, refactoring, and file creation within Leak.la's Suna project. These rules are non-negotiable and must be strictly adhered to.

### I. Core Philosophy & Strategic Alignment (Leak.la):

1.  **Leak.la Mission Alignment:** All contributions to the Suna project must align with Leak.la's broader mission and strategic goals. Consider how changes impact user value, innovation, and operational excellence for Leak.la.
2.  **Quality First:** Prioritize robust, maintainable, and well-documented code. Quality is paramount for Leak.la.
3.  **User-Centricity:** Keep the end-user experience in mind, even for backend changes. How will this impact performance, reliability, or the features Leak.la offers?
4.  **Security as a Foundation:** Security is not an afterthought but a fundamental requirement for all work done on behalf of Leak.la.

### II. General Principles (Enhanced for Leak.la):

1.  **Understand, Align, Act:** Before any modification or creation, thoroughly understand the existing architecture, component responsibilities, data flows, and the specific purpose of the targeted module. Crucially, align the proposed changes with Leak.la's strategic goals for the Suna project. Refer to project documentation (`README.md`, `suna_report.md`, diagrams) and existing code patterns.
2.  **Minimize Scope (with Strategic Exceptions):** Only make changes absolutely necessary for the requested task. Avoid unrelated refactoring or feature creep. However, if a broader refactor is identified that offers significant strategic benefit to Leak.la (e.g., major performance gain, security enhancement, significant reduction in technical debt) and is approved by a human supervisor, it may be undertaken.
3.  **Adhere to and Evolve Patterns:** Strictly follow Leak.la's established coding styles, naming conventions, architectural patterns (FastAPI, Next.js, agent tool design), and directory structures. However, also be prepared to contribute to the evolution of these patterns if improvements are identified, discussed, and approved by the Leak.la team.
4.  **Modularity and Clear Responsibility:** Ensure new functions, components, or services have a clear, single responsibility and are placed in the appropriate module or directory, maintaining the project's organized structure.
5.  **No Unnecessary Creation or Premature Optimization:** Do not create new elements (functions, components, APIs, services) if existing ones can be reasonably adapted or extended. Avoid premature optimization; focus on clarity and correctness first, then optimize if performance issues are identified and verified.
6.  **Configuration Management Excellence:** All new configurations (API keys, URLs, feature flags, thresholds) must be managed through environment variables (`.env` files) and accessed via existing configuration modules (e.g., `backend/utils/config.py`). Never hardcode sensitive information. Ensure configuration is clearly documented for the Leak.la operations team.
7.  **Strategic Dependency Management:** Before adding new dependencies, verify if similar functionality exists or can be achieved with current dependencies. If a new dependency is essential, prioritize well-maintained, secure, and performant libraries that align with Leak.la's preferred tech stack (if specified). Justify its inclusion and add it to the appropriate requirements file (`requirements.txt`, `package.json`).
8.  **Proactive and Transparent Communication:** Report potential issues, ambiguities in requirements, or suggestions for improvement to the human supervisor at Leak.la promptly and clearly. Transparency is key.
9.  **Traceability and Accountability:** Ensure all significant changes are easily traceable. Use clear, descriptive commit messages referencing task/issue IDs if Leak.la uses an issue tracking system. All actions taken should be logged by Cursor for auditability.

### III. Code Implementation and Modification (Leak.la Standards):

1.  **Language Consistency (Strict):** Maintain **English** for all code, internal comments, and commit messages. All user-facing communication (explanations, questions to Leak.la team members) **must be in Brazilian Portuguese**.
2.  **Meaningful Comments:** Write clear, concise comments in English. Focus on *why* a particular approach was taken, explain complex logic, or highlight non-obvious decisions. Update existing comments meticulously if the code they describe changes. Avoid commenting on the obvious.
3.  **API Endpoints (Backend - FastAPI for Leak.la):**
    *   New endpoints must adhere to RESTful principles and Leak.la's API design guidelines (if provided).
    *   Implement comprehensive input validation using Pydantic models and ensure clear OpenAPI schema definitions for all endpoints.
    *   Mandatory authentication and authorization checks for all relevant endpoints, leveraging `backend/utils/auth_utils.py` and Leak.la's security policies.
    *   Place new routers/endpoints logically within existing API modules or create new ones only for distinct new domains of functionality, maintaining structural integrity.
4.  **Agent Tools (Backend - `backend/agent/tools` for Leak.la):
    *   New tools must be self-contained, robust, and clearly define their purpose, OpenAPI schema, and XML representation for LLM interaction.
    *   Tools interacting with the Daytona sandbox must use the `sb_*.py` prefix and adhere strictly to established patterns for sandbox API interaction, prioritizing security.
    *   Tools for external data providers should extend `RapidDataProviderBase` or follow its pattern for RapidAPI, or be placed in `backend/agent/tools/data_providers/`. Ensure all external API usage complies with Leak.la's data handling and third-party service policies.
    *   Grant tools only the minimum necessary permissions. Document any security implications of new tools.
5.  **Frontend Components (Next.js/React for Leak.la):
    *   New components must be reusable, performant, and placed in `frontend/components` or relevant subdirectories. Adhere to Leak.la's UI/UX guidelines and design system (if provided).
    *   Follow established patterns for state management (e.g., React Context, Zustand, Redux, if used by Leak.la), and API calls to the backend.
    *   Strictly use TypeScript; define types for all props, state, and function signatures.
6.  **Database Interactions (Supabase for Leak.la):
    *   All backend database interactions must use the `DBConnection` class in `backend/services/supabase.py` or follow its patterns. No direct, unabstracted database calls from other parts of the application.
    *   Avoid raw SQL queries in business logic if ORM/query builder methods are appropriate and safer. If raw SQL is necessary, it must be reviewed for security (SQLi) and performance.
    *   Database migrations must be handled via the Supabase CLI, with migration scripts version-controlled and reviewed.
7.  **Robust Error Handling & Reporting:** Implement comprehensive error handling. Catch specific exceptions. Return meaningful, user-friendly error messages (in Portuguese, if surfaced to the UI) or structured error codes for APIs. All significant errors must be logged with sufficient context for debugging by the Leak.la team.
8.  **Informative Logging:** Utilize the project's configured logger (`backend/utils/logger.py`) for significant events, errors, and critical decision points. Logs must be structured, informative, and provide context for operational monitoring and debugging by Leak.la.
9.  **Testing is Encouraged:** For Leak.la, any new, non-trivial functionality or significant modification should ideally be accompanied by unit or integration tests. If Cursor cannot write these tests, it **must** clearly identify and report the areas requiring manual testing by the Leak.la development team.
10. **Performance and Scalability Awareness:** Be mindful of the performance and scalability implications of new code or modifications. Highlight any potential bottlenecks or areas that might not scale well under Leak.la's expected load. Query optimization for database interactions is crucial.

### IV. Refactoring (Leak.la Standards):

1.  **Strategic and Justified Refactoring:** Only refactor code if it directly improves the implementation of the current task, addresses identified technical debt that impacts Leak.la's goals (e.g., security, performance), or if explicitly requested and approved by Leak.la.
2.  **No Unapproved Breaking Changes:** Refactoring must not alter existing public APIs or functionality unless this is the specific, approved goal of the task.
3.  **Maintain/Improve Test Coverage:** If tests exist for the code being refactored, ensure they still pass or are updated/expanded accordingly. Refactoring should ideally improve testability.
4.  **Enhance Clarity and Maintainability:** Refactoring efforts should aim to improve the overall clarity, readability, and long-term maintainability of the codebase for the Leak.la team.

### V. File Creation (Leak.la Standards):

1.  **Strict Adherence to Structure:** New files must be created in the correct directory based on their purpose and Leak.la's established project structure.
2.  **Consistent Naming and Boilerplate:** Follow existing naming conventions for files, classes, functions, and variables. Utilize existing boilerplate or templates if available (e.g., for new agent tools, API modules).
3.  **Documentation Stubs for New Modules:** For new significant modules, APIs, or complex functions, create initial documentation stubs (e.g., detailed in-code docstrings in English, or a placeholder in a `docs` folder) outlining purpose, inputs, outputs, and basic usage. This aids future understanding by the Leak.la team.

### VI. Security (Non-Negotiable for Leak.la):

1.  **Comprehensive Input Validation:** Rigorously validate and sanitize all user inputs, data from external APIs, and any data crossing trust boundaries to prevent all forms of injection attacks (SQLi, XSS, command injection, etc.).
2.  **Principle of Least Privilege:** Ensure all components and functions operate with the minimum necessary permissions. New functionality must respect and integrate with Leak.la's existing permission model.
3.  **Secure Secrets Management:** Absolutely no hardcoding of secrets (API keys, passwords, tokens). All secrets must be managed via environment variables and accessed through the designated configuration mechanisms. Leak.la's specific secret management tools or practices must be followed if provided.
4.  **Sandbox Integrity and Security:** When modifying tools interacting with the Daytona sandbox, ensure changes do not compromise the isolation, security, or resource limits of the sandbox environment. All sandbox operations must be logged.
5.  **Data Privacy Adherence (Leak.la Priority):** Be extremely cautious when handling any data that could be user-related, personal, or commercially sensitive. Adhere strictly to Leak.la's data privacy policies and any relevant data protection regulations (e.g., LGPD). Minimize data collection and retention by tools.
6.  **Dependency Vulnerability Management:** Use tools to scan for vulnerabilities in project dependencies if available. Report any identified vulnerabilities in third-party libraries to the Leak.la team for assessment.

### VII. Collaboration, Review, and Version Control (Leak.la Workflow):

1.  **Code Review Readiness:** Ensure all code submitted is well-formatted (using linters/formatters if used by Leak.la), thoroughly commented (in English), and accompanied by clear explanations of changes to facilitate an efficient and effective review process by Leak.la's human developers.
2.  **Version Control Discipline (Git):**
    *   Use clear, descriptive, and concise commit messages in English, following conventional commit formats if adopted by Leak.la (e.g., `feat: ...`, `fix: ...`, `docs: ...`).
    *   Create feature branches for new features or significant changes, following Leak.la's Git branching strategy (e.g., git-flow, GitHub flow).
    *   Keep commits small and atomic where possible.
    *   Rebase or merge branches cleanly as per Leak.la's policy before requesting a final review or merge.

### VIII. Communication and Language Protocol (Reiteration):

1.  **User Interaction Language (Leak.la Team):** All direct communication, explanations of work done, questions for clarification, and reports to any human user or team member at Leak.la **must be in Brazilian Portuguese**.
2.  **Code and Technical Artifacts Language:** All code (variables, function names, class names), comments within the code, commit messages, internal technical documentation (e.g., docstrings, design notes not for end-users), and any other technical artifacts **must be in English**. This ensures global consistency and accessibility of the codebase.

### IX. Continuous Improvement & Knowledge Sharing (Leak.la Culture):

1.  **Constructive Feedback Loop:** Be receptive to feedback from the Leak.la team regarding adherence to these rules, code quality, and overall contributions. Strive to learn and improve continuously.
2.  **Proactive Knowledge Sharing:** If new, beneficial patterns are developed, or significant learnings emerge during tasks (e.g., a more efficient way to use a tool, a security best practice for a specific scenario), document these insights or suggest updates to these rules or Leak.la's internal knowledge base.

By strictly adhering to these enhanced and contextualized rules, the AI assistant (Cursor) will become an invaluable asset to Leak.la, contributing effectively to the Suna project's success while upholding Leak.la's standards for quality, security, and innovation.

# Detailed Report on the Suna Project (kortix-ai/suna)

Introduction

The Suna project, developed by Kortix AI, is an open-source generalist AI agent designed to assist users in performing real-world tasks. Through natural language conversations, Suna acts as a digital companion for research, data analysis, and solving everyday challenges. It combines a robust set of capabilities with an intuitive interface, understanding user needs and delivering effective results. Suna's main functionalities include browser automation for web navigation and data extraction, file management for creating and editing documents, web crawling and extended search capabilities, command-line execution for system tasks, website deployment, and integration with various APIs and services. This report aims to provide an in-depth analysis of the Suna project's architecture, structure, components, and operation, in a way that is understandable to both beginners and experienced programmers.

Suna Project Architecture

Suna is built on a modular architecture, designed to separate concerns and allow for scalability and flexibility. As described in the official documentation and visualized in the project's architecture diagram, the four main components are:

Backend API: A service developed in Python using the FastAPI framework. This component is responsible for managing REST API endpoints, controlling the lifecycle of conversation threads, and integrating with Large Language Models (LLMs), such as those from Anthropic, and others via LiteLLM. It orchestrates business logic and communication between the frontend and the agents.

Frontend: A web application built with Next.js and React. The frontend offers a responsive user interface, including a chat interface for interacting with Suna, a dashboard for information visualization, and other functionalities for the end-user. It communicates with the Backend API to send requests and receive responses.

Agent Docker (Agent Execution Environment): Each Suna agent operates in an isolated execution environment, provisioned through Docker containers managed by the Daytona platform. This secure environment provides the agent with essential capabilities such as browser automation (e.g., via Playwright), a code interpreter, file system access, integration with various tools, and security features. This isolation ensures that tasks executed by agents do not interfere with each other or with the host system.

Supabase Database: Supabase is used for data persistence. It manages user authentication, profile management, conversation history, file storage, agent state, analytics data, and real-time subscriptions. Supabase provides a robust and scalable data layer for Suna.

The official architecture diagram (located at docs/images/diagram.png in the repository) visually illustrates how these components interconnect:

The Frontend (Next.js) interacts directly with the Backend API (FastAPI) and with the Supabase Database (likely for authentication and real-time user data).

The Backend API (FastAPI), in turn, communicates with the Supabase Database for data persistence and with the Agent Docker (Daytona) environment to delegate task execution to agents.

The Agent Docker (Daytona) provides an isolated environment where agents use various tools such as file system access, code interpreter, terminal, browser, and access to data APIs.

This architecture allows Suna to handle a variety of complex tasks, from simple information retrieval to automating workflows involving multiple steps and tools.

Project File and Folder Structure

An analysis of the Suna repository's file and folder structure (https://github.com/kortix-ai/suna) reveals a logical organization that reflects its modular architecture. The repository was cloned, and its detailed structure was mapped to the suna_repo_structure.txt file. Below, we highlight the main directories and files and their respective functions:

/ (Project Root):

.gitignore: Specifies intentionally untracked files and folders by Git.

LICENSE: Contains the project's license (Apache 2.0), defining terms of use and distribution.

README.md: The main documentation file, providing an overview of the project, architecture, use cases, installation and configuration instructions, and acknowledgments. This is the starting point for understanding Suna.

docker-compose.ghcr.yaml: Docker Compose file for using pre-built images from GitHub Container Registry (GHCR), facilitating deployment.

docker-compose.yaml: Docker Compose file for building and running services (backend, frontend, redis) locally from the source code.

/.github/workflows: Contains configuration files for Continuous Integration and Continuous Deployment (CI/CD) workflows using GitHub Actions. The following workflows were identified:

docker-build.yml: Likely responsible for building the project's Docker images.

fly-deploy-PRODUCTION.yml: Defines the deployment process to the production environment on the Fly.io platform.

fly-deploy-STAGING.yml: Defines the deployment process to the staging environment on the Fly.io platform.

update-PROD.yml: Possibly a workflow for updating the application in production.

/backend: Contains all the source code for the backend service.

Dockerfile: Instructions for building the Docker image for the backend service.

api.py: Main entry point of the FastAPI application. Defines application initialization, middlewares (such as CORS and request logging), health check routes, and includes routers for agent, sandbox, and billing APIs. Manages the application lifecycle, including initializing and closing connections to the database (Supabase) and Redis, as well as restoring agents that were running.

.env.example: Example file for environment variables required by the backend (database credentials, API keys for LLMs, Redis, Daytona, etc.).

requirements.txt: Lists the Python dependencies for the backend.

/backend/agent: Contains the core logic for Suna agents.

api.py: Defines FastAPI API routes specific to agent and conversation thread management. Includes endpoints for starting, stopping, getting the status of agent runs, sending messages, and managing agent lifecycles. Also handles authentication and authorization for thread access.

run.py: Contains the main logic for agent execution, including the message processing loop, LLM calls, and tool selection and execution.

/backend/agent/tools: Crucial directory housing the various tools that agents can use to interact with the outside world and perform tasks. Each .py file generally defines one or more tools, with their respective OpenAPI and XML schemas for LLM integration.

__init__.py: May be used to facilitate imports or initializations within the tools module.

computer_use_tool.py: Implements tools for Graphical User Interface (GUI) automation and browser control within the agent's sandbox environment. Includes functionalities like moving the cursor, clicking, scrolling, typing text, and pressing keys. Interacts with an automation service running in the sandbox.

data_providers_tool.py: An aggregator tool that allows agents to interact with various external data providers (APIs). It registers and manages different data providers.

/backend/agent/tools/data_providers: Contains specific implementations for each data provider, such as LinkedinProvider.py, YahooFinanceProvider.py, AmazonProvider.py, ZillowProvider.py, TwitterProvider.py. Each of these files defines how to interact with the corresponding API, usually using a RapidAPI key.

message_tool.py: Provides tools for agent-user communication, such as ask (to ask questions and wait for a response), web_browser_takeover (to request the user to take control of the browser), and complete (to indicate the finalization of all tasks).

sb_browser_tool.py: Specific tools for browser control within the Daytona sandbox, such as navigating to URLs, going back, waiting, clicking elements, inserting text, sending keys, managing tabs, scrolling, getting page content (text and HTML), and saving screenshots. These tools interact with a browser automation service running in the sandbox (likely Playwright).

sb_deploy_tool.py: Tools for deploying applications (static, Next.js, Flask) from the sandbox.

sb_expose_tool.py: Tool for exposing sandbox ports for temporary public access.

sb_files_tool.py: Tools for file manipulation within the sandbox, such as reading, writing, listing files, creating directories, moving, and deleting files.

sb_shell_tool.py: Tool for executing shell commands within the sandbox.

sb_vision_tool.py: Tool for vision capabilities, possibly for analyzing images or the browser screen.

web_search_tool.py: Tool for performing web searches, using providers like Tavily or Firecrawl.

/backend/services: Contains modules for interacting with external services.

billing.py: Logic related to billing and payment status verification.

llm.py: Functions for interacting with LLMs (e.g., Anthropic, OpenAI via LiteLLM).

redis.py: Functions for interacting with Redis (cache, pub/sub for agent control).

supabase.py: DBConnection class for managing the connection to the Supabase database.

/backend/sandbox: Code related to interaction with the Daytona sandbox environment.

api.py: FastAPI routes for managing sandboxes (create, get status, etc.).

sandbox.py: Logic for creating, starting, and managing sandbox instances in Daytona.

/backend/utils: Various utilities.

auth_utils.py: Functions for authentication and authorization (e.g., JWT validation).

config.py: Loads and provides configurations from environment variables.

logger.py: Logging system configuration.

/docs: Contains project documentation.

/docs/images: Images used in documentation, such as diagram.png which illustrates Suna's architecture.

/frontend: Contains all the source code for the frontend application.

Dockerfile: Instructions for building the Docker image for the frontend application.

.env.example: Example file for environment variables required by the frontend (Supabase and backend URLs, public API keys, etc.).

next.config.js: Next.js configuration file.

package.json: Defines frontend project metadata, scripts (like dev, build, start), and Node.js dependencies.

tsconfig.json: TypeScript configuration file for the frontend project.

/frontend/app: Main structure of the Next.js 13+ application (App Router).

Contains layouts, pages, UI components, and routing logic.

/frontend/components: Reusable React components used in the user interface.

/frontend/public: Statically served public files, such as images (e.g., banner.png) and favicons.

/frontend/lib: Utility functions and frontend-specific logic.

/frontend/hooks: Custom React hooks.

/frontend/types: TypeScript type definitions.

This high-level structure demonstrates a clear separation between the backend, frontend, and documentation. Modularity within the backend, especially in the /agent/tools directory, allows for easy addition of new capabilities and integrations. The use of Docker and Docker Compose simplifies the setup and deployment of development and production environments.

Detailed Functioning of Main Components

Next, we will detail the functioning of each of the four main components of Suna's architecture, based on the analysis of the source code and available documentation.

1. Backend API (Python/FastAPI)

The backend is Suna's brain, built with Python and the FastAPI framework, known for its high performance and ease of API development. Its central responsibilities are:

REST Endpoint Management: The backend/api.py file is the entry point of the FastAPI application. It defines the application's configuration, including middlewares for CORS (Cross-Origin Resource Sharing) to allow requests from the frontend, and a middleware to log all HTTP requests, which is crucial for debugging and monitoring. It also includes routers for different API modules:

agent_api.router (from backend/agent/api.py): Handles all requests related to agents, such as starting a new agent run, sending messages to an agent, getting the status of a run, and stopping an agent.

sandbox_api.router (from backend/sandbox/api.py): Manages operations related to Daytona sandboxes, such as creation and possibly status.

billing_api.router (from backend/services/billing.py): Likely handles endpoints related to subscription plans and user payment status.

A health check endpoint (/api/health) is provided to check the application's health.

Thread and Agent Lifecycle Management: The ThreadManager (from the agentpress library) is used to manage conversation threads. The backend/agent/api.py file implements the logic for:

Agent Initiation: Upon receiving a request to start an agent (likely at the /api/thread/{thread_id}/run endpoint), it checks user authentication, billing status, and if an agent is already active for the project. It then creates an agent_run_id, registers the run in the Supabase database with "running" status, and starts the run_agent task in the background (from the backend/agent/run.py file).

Communication with Agents: Allows sending user messages to a running agent and streaming agent responses back to the user. Uses Redis for pub/sub, enabling the frontend to receive real-time updates from the agent.

State and Persistence: The state of agent runs (such as status, errors, responses) is persisted in Supabase. Redis is used to store temporary lists of responses during an agent's execution.

Stopping and Cleanup: Provides endpoints to stop agent runs. The stopping process involves updating the status in the database, publishing a "STOP" signal on relevant Redis channels (global and instance-specific), and clearing associated resources in Redis.

Recovery: On backend startup (lifespan in backend/api.py), there's a routine (agent_api.restore_running_agent_runs) to identify agents that were in "running" status in the database (indicating an unexpected server shutdown) and mark them as "failed," also clearing their Redis resources.

LLM Integration: The backend integrates with Large Language Models (LLMs). The backend/services/llm.py file (though not fully explored here) would be where calls to LLMs (like Anthropic or models via LiteLLM, as mentioned in README.md and .env.example) are made. The agent, during its execution in backend/agent/run.py, formulates prompts for the LLM, sends them, and processes the responses, which may include text or a request to use a tool.

Orchestration and Security: The backend orchestrates the interaction between the frontend, Supabase database, Redis, LLMs, and agent execution environments in Daytona. It also handles authentication (verifying JWTs, as seen in backend/utils/auth_utils.py) and authorization to ensure users can only access their own data and threads.

Configuration Management: Uses the .env file (from .env.example) to load sensitive and environment-specific configurations, such as API keys, database URLs, etc. The backend/utils/config.py module centralizes access to these configurations.

2. Frontend (Next.js/React)

The frontend is the interface with which the user interacts directly. Built with Next.js (a popular React framework) and TypeScript, it offers a modern and responsive user experience.

User Interface: Provides the chat interface where users can converse with Suna, a dashboard to view relevant information (possibly about agent runs, usage, etc.), and pages for authentication and account management.

Communication with the Backend: The frontend makes HTTP requests to the Backend API to:

Authenticate users (likely integrating with Supabase Auth on the client-side and sending JWT tokens to the backend).

Initiate and manage conversations with agents.

Send user messages and receive responses (including real-time streaming of responses).

The frontend/.env.example file shows it needs NEXT_PUBLIC_BACKEND_URL to know where the backend is running.

State Management: Uses React's capabilities and possibly state management libraries to maintain the UI state, such as conversation history, agent run status, etc.

Routing: Next.js provides a file-system-based routing system (within frontend/app for the App Router), facilitating the creation of different pages and navigation.

Build and Deploy: The package.json in the frontend directory contains scripts for development (npm run dev), build (npm run build), and production execution (npm run start). The Dockerfile in the same directory allows containerizing the frontend application for deployment.

3. Agent Docker (Daytona - Agent Execution Environment)

This is one of Suna's most distinctive components, ensuring that each agent executes its tasks in a secure and isolated environment. The Daytona platform is used to provision and manage these environments.

Isolation and Security: Each agent (or each agent run) operates within its own Docker container. This is crucial for security, especially since agents can execute code, interact with the file system, and browse the web. Isolation prevents one agent from affecting others or the host system.

Environment Capabilities: Inside the Daytona container, the agent has access to a set of powerful tools:

Browser Automation: As seen in backend/agent/tools/sb_browser_tool.py, agents can control a browser (likely headless, using Playwright) to navigate websites, extract information, fill forms, take screenshots, etc. This is done via calls to an automation API running inside the sandbox (port 8002 mentioned in the code).

Code Interpreter: Allows the agent to write and execute code (e.g., Python) to perform complex tasks, data analysis, etc.

File System Access: Agents can read, write, and manage files within their isolated environment (sb_files_tool.py).

Shell Command Execution: They can execute terminal commands (sb_shell_tool.py), allowing interactions with the container's operating system.

Tool Integration: The agent can invoke a variety of other tools defined in backend/agent/tools, such as web search, access to data APIs, etc.

Management via Daytona API: The Suna backend interacts with Daytona's API (using DAYTONA_API_KEY and DAYTONA_SERVER_URL from .env) to create, start, and possibly manage these sandboxes. The backend/sandbox/sandbox.py file contains the logic for these interactions. The README.md mentions the need to configure a specific Docker image (adamcohenhillel/kortix-suna:0.0.20) in Daytona with a specific entrypoint (/usr/bin/supervisord).

4. Supabase Database

Supabase is an open-source platform offering a complete backend-as-a-service, built on PostgreSQL. In Suna, it is used for all data persistence needs.

Authentication and User Management: Supabase Auth is used to register new users, log in, and manage user sessions. The frontend interacts directly with Supabase for these flows, and the backend validates JWTs generated by Supabase.

Application Data Persistence:

Conversation History: All messages exchanged between the user and the agent, including agent thoughts and tool usage, are stored. This allows conversations to be resumed and reviewed.

Agent State: Information about agent runs, such as id, thread_id, status (running, completed, failed), created_at, completed_at, error, and responses are stored in the agent_runs table.

Projects and Threads: Suna seems to organize work into "projects" and "threads." The database stores information about these projects and their associated conversation threads.

File Storage: Supabase Storage can be used to store files uploaded by users or generated by agents.

Analytics: May collect data for platform usage analysis.

Real-time Subscriptions: Supabase offers real-time subscriptions, which can be used to notify the frontend about updates (e.g., new agent messages) without constant polling.

Interaction: The backend interacts with Supabase through Supabase's Python library, encapsulated in the DBConnection class in backend/services/supabase.py. Credentials (SUPABASE_URL, SUPABASE_ANON_KEY, SUPABASE_SERVICE_ROLE_KEY) are configured in the .env file.

These four components work together to provide Suna's full functionality. A typical user interaction flow involves the frontend sending a message to the backend, the backend starting or continuing an agent thread, the agent (possibly using an LLM and tools within its Daytona sandbox) processing the message and generating a response, and the backend sending this response back to the frontend, with all relevant data being persisted in Supabase.

Agents, Tools, and APIs

One of Suna's central aspects is its ability to use agents equipped with a diverse set of tools to interact with the digital environment and perform tasks. Analysis of the backend/agent/tools directory and related files reveals a rich ecosystem of functionalities.

Suna Agents

The agent execution logic primarily resides in backend/agent/run.py. A Suna agent operates in a cycle that generally involves:

Receiving a Task/Prompt from the User: Through the chat interface, the user assigns a task to Suna.

LLM Processing: The backend sends the user's prompt, along with conversation history and a list of available tools (with their schemas), to a Large Language Model (LLM).

Reasoning and Tool Selection (Tool Use): The LLM analyzes the task and decides the next step. This could be generating a direct textual response or deciding to use one of the available tools to gather information, interact with a system, or perform an action.

Tool Execution: If the LLM decides to use a tool, the backend executes the corresponding tool with parameters provided by the LLM. Tools often operate within the secure Agent Docker (Daytona) environment.

Observation of Tool Result: The result of the tool execution (an observation) is returned to the LLM.

Iteration: The LLM processes the observation and decides the next step, which could be using another tool, refining the approach, or generating a final response for the user.

Response to User: Eventually, the agent (via the LLM) formulates a final response or report, which is sent back to the user through the frontend.

Suna also supports agent "thoughts" (enable_thinking in AgentStartRequest in backend/agent/api.py), which are intermediate LLM reasoning steps, visible to the user, showing how the agent is approaching the problem.

Detailed Tools

Tools are Python modules located in backend/agent/tools that provide specific functionalities to agents. Each tool usually defines an OpenAPI schema and an XML schema, which describe to the LLM how the tool should be used (name, description, parameters). The main identified tools are:

ComputerUseTool (computer_use_tool.py): This tool enables Graphical User Interface (GUI) automation within the sandbox. It interacts with an automation service (likely a VNC server or similar with an API) running in the Daytona container. Functionalities include:

move_to(x, y): Moves the mouse cursor to specific coordinates.

click(x, y, button, num_clicks): Performs mouse clicks at specific coordinates or the current position.

scroll(amount): Simulates mouse scrolling.

typing(text): Types the specified text.

press(key): Presses a keyboard key (e.g., 'enter', 'ctrl+c').

wait(duration): Pauses execution for a period.

mouse_down(button, x, y): Presses a mouse button.

mouse_up(button, x, y): Releases a mouse button.

screenshot(path, x, y, width, height): Captures a screenshot of a specific area or the entire screen.

get_screen_size(): Gets screen dimensions.

find_text_on_screen(text): Tries to locate text on the screen (OCR).

find_image_on_screen(image_path): Tries to locate an image on the screen.

DataProvidersTool (data_providers_tool.py): Acts as a gateway to various external data providers, mainly through APIs. It allows the agent to discover and execute calls to specific endpoints of these providers.

get_data_provider_endpoints(service_name): Returns available endpoints for a specific data provider (e.g., 'linkedin', 'twitter').

execute_data_provider_call(service_name, route, payload): Executes a call to a specific endpoint of a data provider.

Data Providers (backend/agent/tools/data_providers/): Each file in this subdirectory implements the logic for interacting with a specific API. Examples include:

LinkedinProvider.py: For interacting with LinkedIn data (via RapidAPI).

YahooFinanceProvider.py: For fetching financial data from Yahoo Finance (via RapidAPI).

AmazonProvider.py: For interacting with Amazon data (via RapidAPI).

ZillowProvider.py: For obtaining real estate data from Zillow (via RapidAPI).

TwitterProvider.py: For interacting with Twitter (via RapidAPI).

RapidDataProviderBase.py: A base class to facilitate creating new providers that use RapidAPI.

MessageTool (message_tool.py): Facilitates interactive communication between the agent and the user.

ask(text, attachments): Sends a question to the user and waits for a response. Crucial for clarifying ambiguities, requesting additional information, or obtaining confirmations.

web_browser_takeover(text, attachments): Requests the user to take control of the browser for tasks the agent cannot perform automatically (e.g., solving CAPTCHAs, complex logins).

complete(): A special tool the agent uses to indicate that all tasks have been successfully completed.

(An inform tool was commented out in the code, suggesting a feature for sending non-blocking notifications to the user).

SandboxBrowserTool (sb_browser_tool.py): Provides a comprehensive set of functionalities for controlling a web browser (likely Playwright) running inside the Daytona sandbox. Interacts with a browser automation API on port 8002 of the sandbox.

browser_navigate_to(url): Navigates to a specific URL.

browser_go_back(): Goes back to the previous page in the browser history.

browser_wait(seconds): Pauses browser execution.

browser_click_element(index): Clicks an element on the page identified by an index (likely obtained from a previous page analysis).

browser_input_text(index, text): Inserts text into a page element.

browser_send_keys(keys): Sends keys to the browser (e.g., 'Enter', 'Control+a').

browser_switch_tab(page_id): Switches to a specific browser tab.

browser_close_tab(page_id): Closes a browser tab.

browser_scroll_page(direction): Scrolls the page up or down.

browser_get_page_content(ocr, text_only): Gets the current page content (HTML, extracted text, or text via OCR).

browser_take_screenshot(path): Saves a screenshot of the current page.

SandboxDeployTool (sb_deploy_tool.py): Tools for deploying applications from the sandbox.

deploy_static_website(local_dir): Deploys a static website.

deploy_nextjs_app(local_dir): Deploys a Next.js application.

deploy_flask_app(local_dir): Deploys a Flask application.

SandboxExposeTool (sb_expose_tool.py): Allows exposing sandbox ports for temporary public access.

expose_port(port): Makes a local sandbox port publicly accessible.

SandboxFilesTool (sb_files_tool.py): Manages files and directories within the sandbox.

file_read(path, start_line, end_line): Reads the content of a file.

file_write(path, content, append): Writes or appends content to a file.

file_list(path): Lists files and directories in a path.

file_create_directory(path): Creates a new directory.

file_move(source_path, destination_path): Moves or renames a file/directory.

file_delete(path): Deletes a file or directory.

SandboxShellTool (sb_shell_tool.py): Allows execution of shell commands in the sandbox.

shell_exec(command, exec_dir, timeout): Executes a shell command and returns the output.

SandboxVisionTool (sb_vision_tool.py): Vision capabilities for the agent.

vision_describe_image(image_path, prompt): Describes an image using a vision model.

vision_read_text_from_image(image_path): Extracts text from an image (OCR).

WebSearchTool (web_search_tool.py): Performs web searches.

search_web(query, engine): Executes a web search using a configured search engine (e.g., Tavily, Firecrawl, Google Search API).

crawl_webpage(url): Extracts the main content from a webpage.

External APIs Used

Suna integrates with a series of external APIs to expand its capabilities:

LLMs (Large Language Models):

Anthropic (Claude): Mentioned as a primary LLM provider.

OpenAI (GPT): Supported, as seen in configurations and model aliases.

LiteLLM: Used as an abstraction layer to interact with various LLMs from different providers (e.g., Google Gemini, Grok, Deepseek via OpenRouter).

API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY, GROQ_API_KEY, OPENROUTER_API_KEY) are configured in the backend's .env file.

Supabase: Used as the database and authentication backend. Communication is via the Supabase client library, configured with SUPABASE_URL and API keys.

Redis: Used for caching, session management, and as a message broker (pub/sub) for real-time communication between the backend and agents, and for streaming responses to the frontend.

Daytona: Platform for provisioning and managing Dockerized sandbox environments where agents execute. Integration is via Daytona's API, configured with DAYTONA_API_KEY and DAYTONA_SERVER_URL.

Web Search Providers:

Tavily: Search API optimized for LLMs (TAVILY_API_KEY).

Firecrawl: API for crawling and extracting data from websites (FIRECRAWL_API_KEY).

RapidAPI: Used as a hub to access various third-party data APIs (LinkedIn, Yahoo Finance, Amazon, Zillow, Twitter). Requires a RAPID_API_KEY and individual subscription to each API on the RapidAPI platform.

AWS (Amazon Web Services): The variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME in .env.example suggest possible integration with AWS services, although specific use was not detailed in the superficial analysis (could be for S3, logging, etc.).

This set of tools and integrations with external APIs gives Suna great versatility to tackle a wide range of tasks, from research and data analysis to process automation and interaction with complex systems.

Use Cases and Practical Examples

The Suna project's README.md presents a list of use cases that demonstrate the platform's versatility and power. These examples illustrate how Suna can be applied to solve real problems and automate complex tasks. Below, we analyze some of these cases and how Suna's tools and architecture make them possible:

Competitor Analysis: "Analyze the market for my next company in the healthcare industry, located in the UK. Provide key players, market size, strengths and weaknesses, and add URLs of their websites. At the end, generate a PDF report."

Tools Involved: WebSearchTool (to find information about companies, market), SandboxBrowserTool (to navigate company websites, extract specific data), SandboxFilesTool (to create and organize collected data, and to generate the PDF report, possibly using a Python library within the sandbox).

Agent Flow: The agent would use web search to identify healthcare companies in the UK. Then, it would visit these companies' websites using the browser to collect detailed information. Data would be organized, and finally, a PDF generation tool (executed in the sandbox) would be used to create the report.

VC (Venture Capital) List: "Provide the list of the most important VC Funds in the United States based on Assets Under Management. Provide website URLs and, if possible, a contact email."

Tools Involved: WebSearchTool, SandboxBrowserTool (to find VC lists, visit their sites, and extract contact information), SandboxFilesTool (to save the list).

Agent Flow: Similar to the previous one, involving search, navigation, and extraction of specific data (AUM, website, email).

LinkedIn Candidate Search: "Go on LinkedIn and find 10 available profiles... for a junior software engineer position... in Munich, Germany..."

Tools Involved: DataProvidersTool (specifically the LinkedinProvider via RapidAPI) or, alternatively, SandboxBrowserTool (to perform searches on LinkedIn and extract profiles, although direct LinkedIn automation is complex and may violate terms of service, making the API a more likely route if functional).

Agent Flow: The agent would use the LinkedIn access tool to filter candidates based on the provided criteria and extract profile information.

Corporate Trip Planning: "Generate a route plan for my company... California... 8 people... 7 days... check weather forecast..."

Tools Involved: WebSearchTool (to research flights, accommodations, activities, weather forecast), SandboxBrowserTool (to interact with booking sites or get details), SandboxFilesTool (to create the itinerary).

Agent Flow: The agent would research transportation options, hotels, activities suitable for a group, check the weather forecast to plan indoor/outdoor activities, and compile everything into a detailed plan.

Working with Excel Spreadsheets: "...set up an Excel spreadsheet with all information about Italian lottery games... generate and send me a spreadsheet..."

Tools Involved: WebSearchTool (to find public information about lotteries), SandboxFilesTool (to create and manipulate the Excel file, possibly using Python libraries like pandas or openpyxl within the sandbox via SandboxShellTool or code interpreter).

Agent Flow: The agent would collect data online and then use file manipulation and code tools to populate the Excel spreadsheet.

Automated Speaker Sourcing for Events: "Find 20 speakers on AI ethics from Europe... who have spoken at conferences in the last year... scrape conference websites, cross-reference on LinkedIn and YouTube..."

Tools Involved: WebSearchTool, SandboxBrowserTool (to scrape conference websites, YouTube), DataProvidersTool (for LinkedIn), SandboxFilesTool.

Agent Flow: This is a complex use case demonstrating Suna's ability to orchestrate multiple tools to collect, cross-reference, and summarize information from various sources.

Other mentioned use cases, such as Summarizing and Cross-Referencing Scientific Articles, B2B Client Research + First Contact Draft, SEO Analysis, Personal Trip Generation, Recently Funded Startups, and Scraping Forum Discussions, follow similar patterns, combining web search, navigation/data extraction from specific sites, use of data APIs, and file manipulation to produce the desired outcome.

Suna's ability to perform these tasks lies in its architecture, which allows for:

Natural Language Understanding: To understand the user's complex request.

Planning: The LLM needs to break down the task into executable sub-tasks.

Tool Selection and Use: Choosing the right tool for each sub-task.

Secure Environment Execution: The Daytona sandbox allows tools like web browsing and code execution to be done safely.

Iteration and Aggregation: Collecting information from multiple sources and aggregating it into a cohesive final result.

Suna Agent Execution Flow

Understanding the Suna agent's execution flow is fundamental to understanding how it transforms a user request into a concrete result. Although exact details may vary depending on the task, a general flow can be described as follows:

Interaction Initiation (Frontend): The user types a request into the Suna frontend chat interface.

Request to Backend: The frontend sends the user's message, along with the thread_id (conversation identifier) and authentication token, to an endpoint in the Backend API (e.g., /api/thread/{thread_id}/message).

Authentication and Authorization (Backend): The backend verifies the user's JWT token and ensures they have permission to access the specified thread_id.

Active Agent and Billing Check (Backend - backend/agent/api.py): The system checks if an agent run (agent_run) is already active for this thread or project. It may also check the user's billing status.

Thread Creation or Retrieval (Backend - ThreadManager): The ThreadManager loads the conversation history for the provided thread_id from Supabase.

Agent Execution Start (Backend - backend/agent/api.py): If a new agent processing cycle is needed:

A new agent_run_id is generated.

The run status is set to "running" in Supabase.

An asynchronous task run_agent(thread_id, agent_run_id, user_input, config) is initiated (logic in backend/agent/run.py).

The backend may start streaming status/thought events to the frontend via Redis Pub/Sub and, subsequently, Server-Sent Events (SSE) or WebSockets.

Agent Main Loop (backend/agent/run.py):

LLM Prompt Preparation: The agent constructs a prompt for the LLM. This prompt usually includes:

A system message defining the agent's role, capabilities, and how it should behave (e.g., how to use tools, response format).

The current conversation history.

The user's latest message/task.

A list of available tools, with their descriptions and schemas (OpenAPI/XML) so the LLM knows how to request them.

LLM Call (backend/services/llm.py): The prompt is sent to the configured LLM (Anthropic, OpenAI, etc., via LiteLLM).

LLM Response Processing: The LLM's response is received. It can be:

Direct textual response: If the LLM believes it can respond directly to the user.

Tool Call Request: If the LLM decides it needs a tool. The LLM's response will indicate the tool name and parameters to use (usually in XML or JSON format, depending on prompt configuration).

Thinking Steps: The LLM may generate "thoughts" or intermediate reasoning steps, which can be sent to the frontend to keep the user informed of progress.

Tool Execution (If Requested):

The agent identifies the requested tool (e.g., WebSearchTool, SandboxBrowserTool).

Parameters are extracted from the LLM's response.

The function corresponding to the tool is executed. Many tools that interact with the external environment (browser, file system, shell) are executed within the Daytona sandbox. This involves the Suna backend communicating with the Daytona sandbox API or services running inside the agent's container.

The result of the tool execution (an "observation") is captured.

Return to LLM with Observation: If a tool was used, the resulting observation is added to the conversation history (or a temporary context) and sent back to the LLM in the next cycle, so it can process the result and decide the next step.

Iteration: The loop continues (LLM -> Tool -> Observation -> LLM...) until the LLM determines the task is complete or that it needs more information from the user (in which case, it would use the ask tool from MessageTool).

Sending Final/Intermediate Response (Backend): Textual responses from the LLM, (formatted) tool results, or status messages are sent to the frontend, usually via streaming for an interactive experience.

Database State Update (Backend): The agent run (agent_run) status is updated in Supabase to "completed," "failed" (with an error message), or "stopped." Final results or a summary may be stored.

Resource Cleanup (Backend): Temporary resources, such as Redis keys related to the active run, may be cleared or have a TTL set.

Display on Frontend: The frontend receives messages and updates and displays them in the chat interface for the user.

This flow highlights Suna's iterative and tool-oriented nature, where the LLM acts as an intelligent orchestrator, deciding when and how to use the tools at its disposal to fulfill the user's request.

Configuration and Installation (Self-Hosting)

The README.md provides detailed instructions for configuring and self-hosting Suna locally. The process involves setting up several components and dependencies:

Main Requirements:

A Supabase project (for database and authentication).

A Redis database (for cache and session/message management).

A Daytona sandbox environment (for secure agent execution).

Python 3.11 for the backend.

API keys for LLM providers (e.g., Anthropic, OpenAI).

Optional API keys for Tavily (enhanced search) and Firecrawl (web scraping).

RapidAPI key (for data services like LinkedIn, Yahoo Finance, etc.).

Prerequisite Steps:

Supabase: Create a Supabase project, get API URL, anonymous key (anon_key), and service role key (service_role_key). Install Supabase CLI.

Redis: Set up a Redis instance (Upstash for cloud, local installation, or via provided Docker Compose).

Daytona: Create a Daytona account, generate an API key. Add the Docker image adamcohenhillel/kortix-suna:0.0.20 with the entrypoint /usr/bin/supervisord -n -c /etc/supervisor/conf.d/supervisord.conf.

LLM API Keys: Obtain keys for Anthropic and/or other providers via LiteLLM.

Search API Keys (Optional): Obtain keys for Tavily and Firecrawl.

RapidAPI Key (Optional): Obtain a RapidAPI key and subscribe to individual API services to be used (e.g., LinkedIn Data Scraper).

Installation Steps:

Clone Repository: git clone https://github.com/kortix-ai/suna.git and enter the suna directory.

Configure Backend Environment:

Navigate to backend.

Copy .env.example to .env.

Fill in all credentials and API keys in the .env file.

Configure Supabase Database:

Log in to Supabase CLI: supabase login.

Link project: supabase link --project-ref YOUR_PROJECT_REF.

Apply database migrations: supabase db push.

Ensure the basejump schema is exposed in Supabase data API settings.

Configure Frontend Environment:

Navigate to frontend.

Copy .env.example to .env.local.

Fill in NEXT_PUBLIC_SUPABASE_URL, NEXT_PUBLIC_SUPABASE_ANON_KEY, NEXT_PUBLIC_BACKEND_URL (e.g., http://localhost:8000/api for local development, or http://backend:8000/api if using Docker Compose), and NEXT_PUBLIC_URL (e.g., http://localhost:3000).

Install Dependencies:

Frontend: cd frontend && npm install.

Backend: cd ../backend && pip install -r requirements.txt.

Start Application (Without Docker Compose):

Start Frontend: In one terminal, cd frontend && npm run dev.

Start Backend: In another terminal, cd backend && python api.py.

Alternative with Docker Compose:

Ensure .env (backend) and .env.local (frontend) files are correctly configured (especially REDIS_HOST=redis in backend and NEXT_PUBLIC_BACKEND_URL="http://backend:8000/api" in frontend).

To use pre-built images from GHCR: export GITHUB_REPOSITORY="your-github-username/repo-name" && docker compose -f docker-compose.ghcr.yaml up.

To build images locally: docker compose up.

Docker Compose already includes a Redis service.

Access Suna: Open browser to http://localhost:3000.

The instructions are comprehensive and cover both manual component execution and Docker Compose usage for a more streamlined setup. The need for multiple API keys and configuration of external services like Supabase and Daytona indicates that installation, though well-documented, requires attention to several details.

Security Considerations

Security is a critical aspect for a generalist AI agent like Suna, which can execute code, browse the web, and manipulate files. Suna's architecture incorporates several security considerations:

Agent Execution Environment Isolation: The primary security measure is the use of Docker containers managed by Daytona for each agent. This isolates the agent's execution environment from the host system and other agents. Any code executed or file manipulated by the agent is restricted to this container.

Tool Control: Agents do not have unrestricted system access. Their capabilities are defined by the tools made available to them. The code for these tools is part of the Suna backend and can be audited.

Authentication and Authorization: Access to Suna's API and user data is protected by authentication (via Supabase Auth and JWT tokens). The system verifies if the user has permission to access specific threads and projects.

API Key Management: Using .env files to store API keys and other sensitive credentials is a standard practice to prevent hardcoding this information into the source code. It's crucial that these .env files are properly secured in the production environment.

Secure Communication: Presumably, communication between the frontend, backend, and external services (Supabase, Daytona, LLMs) should be over HTTPS/TLS to protect data in transit.

Input Validation: Although not explicitly detailed in the analysis, input validation for API-received data is expected to prevent common vulnerabilities like code injection or XSS, especially in interactions with tools that execute commands or generate content.

Resource Limitation (Potential): In production environments, it may be necessary to implement resource limitations for agent containers (CPU, memory, execution time) to prevent abuse or denial of service.

Potential Security Challenges:

LLM Prompt Injection: As Suna relies on LLMs to interpret user requests and decide on tool usage, it may be susceptible to prompt injection attacks, where a malicious user tries to trick the LLM into executing unauthorized actions.

Tool Security: If a tool has a vulnerability (e.g., a flaw in the shell execution tool allowing escape from the expected directory), it could compromise the agent's sandbox.

Data Leakage via Browsing: If an agent is instructed to browse malicious websites or enter sensitive data into untrusted forms, it could lead to risks.

Overall, the approach of using Daytona sandboxes for agent execution is a strong security measure. Continuous auditing of tool code and implementation of security best practices across all components are essential.

Conclusion

The Suna project (kortix-ai/suna) represents a significant and well-architected effort to create an open-source generalist AI agent. Its modular architecture, comprising a Next.js/React frontend, a Python/FastAPI backend, a Supabase database, and, crucially, isolated agent execution environments via Daytona, provides a robust and flexible foundation.

Suna's strength lies in its ability to integrate Large Language Models with a diverse and powerful set of tools. These tools allow agents to interact with the digital world in complex ways, including advanced web navigation, GUI automation, code execution, file manipulation, interaction with data APIs, and more. The use cases demonstrate Suna's potential to automate research, analysis, planning, and interaction with online services.

The documentation, especially the README.md, is clear and provides good instructions for developers wishing to understand, install, and contribute to the project. The code structure is generally well-organized, facilitating navigation and understanding of the system's different parts.

For beginner users, Suna offers an intuitive chat interface for delegating tasks. For experienced programmers, the open and modular architecture, along with the clarity of Python and TypeScript code, offers ample opportunities for extension, customization, and contribution. The self-hosting capability is a major attraction for users and businesses seeking greater control over their data and AI infrastructure.

In summary, Suna is a promising project in the growing field of AI agents, with a solid technical foundation and great potential to evolve into an even more powerful and versatile tool.

References

Suna GitHub Repository: https://github.com/kortix-ai/suna

Documentation and source code contained within the repository, including README.md, configuration files, and code from the backend, frontend, and .github directories.

Additional Diagrams of the Suna Project Structure

As requested, additional diagrams have been generated to provide different perspectives on the Suna project's architecture and operation. These diagrams were created using PlantUML and aim to complement the official diagram provided in the repository.

1. Component Diagram (Detailed View)

This diagram focuses on the main software components of the Suna system and their direct interactions. It expands on the original diagram's view, detailing the internal elements of each main component a bit more.

Purpose: To show the software's building blocks and how they connect to form the complete system.

(Image: /home/ubuntu/diagrams/suna_component_diagram_v2.png)

Description of Components in the Diagram:

Frontend (Next.js/React): Responsible for the user interface (UI), including chat and dashboard.

Backend API (Python/FastAPI): Orchestrates business logic, thread management, LLM integration, and communication with other services.

Agent Execution Environment (Daytona): Platform hosting isolated Docker containers for each agent. Within each container, the agent has its own logic, browser automation tools, code interpreter, file access, and shell.

Supabase Database (PostgreSQL): Stores persistent data like user authentication, conversation history, agent state, and files.

External Services: Represents third-party services used, such as LLM Providers, Search APIs (Tavily), Scraping APIs (Firecrawl), and Data Providers (via RapidAPI).

Redis: Used for caching, session management, and as a message broker for real-time communication.

Main Interactions Shown:

The Frontend sends requests to the Backend.

The Backend interacts with the Database, Agent Environment, Redis, and External Services.

The Agent Environment can, indirectly through the backend or directly (if configured), interact with External Services and Redis.

The Frontend also interacts with Supabase for authentication and real-time subscriptions.

2. Data Flow Diagram (DFD) - Level 0 (Context)

This Level 0 Data Flow Diagram (also known as a context diagram) shows the Suna system as a single process and its interactions with external entities (users and other systems).

Purpose: To provide a high-level view of the system's scope and how it exchanges information with the outside world.

(Image: /home/ubuntu/diagrams/suna_dfd_level0_v2.png)

External Entities and Main Data Flows:

User: Interacts with the Suna System by sending requests (text, files) and receiving responses (text, files, UI updates).

Suna AI Agent System: The central process that receives user requests and orchestrates actions.

LLM Providers: Receives prompts and tool specifications from the Suna system; sends back LLM responses and tool calls.

Web Resources: Receives search queries, API calls, and navigation commands; sends back search results, API data, and page content.

Daytona Platform: Receives commands for sandbox creation and task execution; sends back sandbox status and execution results.

Supabase: Receives requests to store/retrieve data; sends back queried data and authentication status.

Redis: Receives requests to store/retrieve cache and publish/subscribe messages; sends back cached data and subscribed messages.

3. Sequence Diagram (Typical Agent Interaction)

This sequence diagram illustrates the interactions between Suna's different system components during a typical flow of processing a user's message or task.

Purpose: To show the temporal order of messages and calls between components to perform a specific functionality.

(Image: /home/ubuntu/diagrams/suna_sequence_diagram.png)

Main Steps of the Sequence:

The User sends a message/task via the Frontend.

The Frontend forwards the request to the Backend API.

The Backend verifies authentication and permissions with the Supabase DB.

If authorized, the Backend retrieves/updates thread history (using Redis and/or Supabase).

The Backend initiates an Agent Run and sends a prompt to the LLM.

The LLM responds with direct text or a Tool Call request.

If it's a direct response, the Backend sends it to the Frontend.

If it's a Tool Call, the Backend delegates tool execution to the Agent Execution Environment (Daytona).

The AgentEnv executes the tool and returns the result (observation) to the Backend.

The Backend sends the observation back to the LLM for the next step.

The LLM-Tool-Observation cycle repeats until the task is completed.

The final response is sent to the Frontend and displayed to the User.

The Backend updates the run status in the Supabase DB.

4. Deployment Diagram

This diagram shows the physical architecture and how software components are distributed and deployed on hardware nodes or execution environments.

Purpose: To illustrate the system's topology in terms of infrastructure.

(Image: /home/ubuntu/diagrams/suna_deployment_diagram_v2.png)

Deployment Components:

User Device (Browser): Where the user accesses the frontend application.

Cloud Infrastructure (e.g., Fly.io or Self-Hosted): The main hosting environment.

Frontend Server (Next.js): Hosts the Suna Frontend application.

Backend API Server (FastAPI): Hosts the Suna Backend API.

Redis Server: Redis instance for cache and messaging.

Supabase Platform: Platform providing the PostgreSQL database, authentication, storage, and real-time services.

Daytona Platform: Platform managing and orchestrating agent Docker containers (Sandboxes).

External API Services: Third-party services accessed over the internet (LLMs, Search/Scraping APIs, Data APIs).

Main Network Connections:

The user's device connects to the Frontend Server via HTTPS.

The Frontend Server communicates with the Backend Server (HTTP/S).

The Backend Server connects to Redis, Supabase, Daytona, and External API Services via appropriate network connections (TCP/IP, HTTPS, database connections).

These diagrams offer different levels of abstraction and focus on distinct aspects of Suna's architecture. It is hoped that, together, they provide a more complete and multifaceted understanding of the project.
